{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "import sys\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "your_list= []\n",
    "file1_1 = open(\"reviews.csv\",\"r\",encoding=\"utf8\")\n",
    "for line1_1 in file1_1: \n",
    "    your_list.append(line1_1[:-1])\n",
    "\n",
    "file1_1.close()\n",
    "your_list = your_list[1:]\n",
    "your_list = your_list[0:1000]\n",
    "your_label   = []\n",
    "your_comment = []\n",
    " \n",
    "for i in your_list:\n",
    "    your_label.append(i.split('|')[0])\n",
    "    your_comment.append(i.split('|')[1:])\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text\n",
    "    \n",
    "your_comment_1 = your_comment\n",
    "   \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs  = np.array(your_comment_1)\n",
    "bag   = count.fit_transform(docs.ravel())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "your_sentiment = []\n",
    "\n",
    "for i in your_label:\n",
    "    if i=='positive': your_sentiment.append(1)\n",
    "    else: your_sentiment.append(0)\n",
    "\n",
    "x_train = np.asarray(your_comment_1[0:800])\n",
    "y_train = np.asarray(your_sentiment[0:800])\n",
    "x_test  = np.asarray(your_comment_1[800:])\n",
    "y_test  = np.asarray(your_sentiment[800:])\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text=str(text)\n",
    "    return text.split()\n",
    "def tokenizer_porter(text):\n",
    "    text=str(text)\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter]},\n",
    "#               'clf__max_depth': [1, 5, 10]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None]}\n",
    "#               'clf__max_depth': [1, 5, 10]},\n",
    "              ]\n",
    "dt_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf',  AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),\n",
    "                                                 n_estimators=600,\n",
    "                                                 learning_rate=1.5,\n",
    "                                                 algorithm=\"SAMME\"))])\n",
    "\n",
    "gs_dt_tfidf = GridSearchCV(dt_tfidf, \n",
    "                           param_grid, \n",
    "                           cv=5,\n",
    "                           verbose=1)\n",
    "\n",
    "\n",
    "train_sizes, train_scores, test_scores =\\\n",
    "                learning_curve(estimator=gs_dt_tfidf,\n",
    "                               X=x_train,\n",
    "                               y=y_train,\n",
    "                               train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                               cv=10,\n",
    "                               n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/learning_curve.png', dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs_dt_tfidf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameter set: %s ' % gs_dt_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_dt_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf  =  gs_dt_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "gs_dt_tfidf.fit(x_train, y_train)\n",
    "y_pred = gs_dt_tfidf.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
